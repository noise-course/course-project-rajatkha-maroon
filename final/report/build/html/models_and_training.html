<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>5. Models and training &#8212; netML Malware Classification 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=2709fde1"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="6. netML Autogluon" href="reproduction_netml.html" />
    <link rel="prev" title="4. Dimensionality reduction with PCA" href="pca_dim_reduction.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="models-and-training">
<h1>5. Models and training<a class="headerlink" href="#models-and-training" title="Link to this heading">¶</a></h1>
<p>With the PCA-reduced datasets, I now train several classical ML models without running into memory issues.</p>
<section id="train-and-test-splits">
<h2>5.1. Train and test splits<a class="headerlink" href="#train-and-test-splits" title="Link to this heading">¶</a></h2>
<p>I perform a <strong>single stratified train/validation split</strong> for each task:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train_easy</span><span class="p">,</span> <span class="n">X_val_easy</span><span class="p">,</span> <span class="n">y_train_easy</span><span class="p">,</span> <span class="n">y_val_easy</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_easy</span><span class="p">,</span> <span class="n">y_easy</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y_easy</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="n">X_train_hard</span><span class="p">,</span> <span class="n">X_val_hard</span><span class="p">,</span> <span class="n">y_train_hard</span><span class="p">,</span> <span class="n">y_val_hard</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_hard</span><span class="p">,</span> <span class="n">y_hard</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y_hard</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="models">
<h2>5.2. Models<a class="headerlink" href="#models" title="Link to this heading">¶</a></h2>
<p>I evaluate the following models:</p>
<ol class="arabic simple">
<li><p>Random forest</p></li>
<li><p>LogReg</p></li>
<li><p>SGD LogReg</p></li>
</ol>
</section>
<section id="evaluation">
<h2>5.3. Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">¶</a></h2>
<p><strong>Easy results:</strong></p>
<p>task         model      acc  balanced_acc  precision_macro  recall_macro  f1_macro</p>
<p>0  easy  RandomForest  0.99997      0.999942         0.999980      0.999942  0.999961</p>
<p>1  easy        LogReg  0.99994      0.999909         0.999934      0.999909  0.999922</p>
<p>2  easy    SGD_logreg  0.99994      0.999909         0.999934      0.999909  0.999922</p>
<p><strong>Hard results:</strong></p>
<p>task         model       acc  balanced_acc  precision_macro  recall_macro  f1_macro</p>
<p>0  hard  RandomForest  0.884492      0.703461         0.707550      0.703461  0.705189</p>
<p>1  hard        LogReg  0.832160      0.583881         0.669906      0.583881  0.594641</p>
<p>2  hard    SGD_logreg  0.796339      0.522495         0.587273      0.522495  0.525238</p>
</section>
<section id="interpretation-for-easy-class">
<h2>5.3. Interpretation for easy class<a class="headerlink" href="#interpretation-for-easy-class" title="Link to this heading">¶</a></h2>
<p>All three models achieve virtually perfect performance (balanced accuracy ≥ 0.9999).
This indicates:</p>
<ol class="arabic simple">
<li><p>The PCA representation preserves almost all the necessary information needed to distinguish benign from malware flows.</p></li>
<li><p>The binary decision boundary is extremely well-structured, meaning that even linear models (LogReg, SGD) find a clean separation between the two classes.</p></li>
<li><p>The dataset for the easy class is highly linearly separable.</p></li>
</ol>
</section>
<section id="interpretation-for-hard-class">
<h2>5.4. Interpretation for hard class<a class="headerlink" href="#interpretation-for-hard-class" title="Link to this heading">¶</a></h2>
<p>Results are significantly lower than the easy task.</p>
<p>Key observations:</p>
<ol class="arabic simple">
<li><p>Random Forest performs best on the hard task: achieves 88% accuracy, but only 70% balanced accuracy. This gap means the model favors the majority families and struggles on minority ones.</p></li>
<li><p>Linear models perform noticeably worse. Logistic Regression: 83% accuracy but only 58% balanced accuracy. SGD: drops further to ~52% balanced accuracy. This indicates that malware families are not linearly separable.</p></li>
<li><p>Macro precision/recall reveal imbalance issues.</p></li>
</ol>
</section>
<section id="testing-other-models-v2">
<h2>5.5. Testing other models (v2)<a class="headerlink" href="#testing-other-models-v2" title="Link to this heading">¶</a></h2>
<p>On the hard class, I also evaluate the following models:</p>
<ol class="arabic simple">
<li><p>ExtraTrees</p></li>
<li><p>HistGB</p></li>
<li><p>SGD_LogReg</p></li>
<li><p>SVM_rbf</p></li>
</ol>
</section>
<section id="evaluation-on-other-models-v2">
<h2>5.6. Evaluation on other models (v2)<a class="headerlink" href="#evaluation-on-other-models-v2" title="Link to this heading">¶</a></h2>
<p>After introducing additional models better suited to nonlinear feature spaces, the hard-task results improved noticeably. The new results are:</p>
<p>Model	             Accuracy	Balanced Acc	Precision (macro)	Recall (macro)	F1 (macro)</p>
<p>ExtraTrees	         0.8813	    0.6937	        0.7192	            0.6937	        0.6974</p>
<p>HistGradientBoosting 0.9010	    0.7329	        0.7081	            0.7329	        0.7070</p>
<p>SVM_rbf              0.8825     0.7741          0.7685              0.7741          0.7147</p>
<p>Due to time limit, I could’t run GradientBoost, XGBoost, and others which could have performed slightly better.</p>
</section>
<section id="intepretation-on-other-models-v2">
<h2>5.7. Intepretation on other models (v2)<a class="headerlink" href="#intepretation-on-other-models-v2" title="Link to this heading">¶</a></h2>
<p>The best model is the SVM with RBF kernel, which achieves 0.7741 balanced accuracy, the highest among all tested methods. Unlike tree models, the RBF kernel constructs curved decision boundaries and is better able to isolate non-linearly separable data. This improves balanced accuracy. The SVM result is also the closest to the official NetML leaderboard performance, indicating that the above methods can partially compensate for the information loss introduced by PCA.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">netML Malware Classification</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_and_flows.html">2. Dataset and flow extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="features_nprint.html">3. Feature Generation with nPrint</a></li>
<li class="toctree-l1"><a class="reference internal" href="pca_dim_reduction.html">4. Dimensionality reduction with PCA</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">5. Models and training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#train-and-test-splits">5.1. Train and test splits</a></li>
<li class="toctree-l2"><a class="reference internal" href="#models">5.2. Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation">5.3. Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interpretation-for-easy-class">5.3. Interpretation for easy class</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interpretation-for-hard-class">5.4. Interpretation for hard class</a></li>
<li class="toctree-l2"><a class="reference internal" href="#testing-other-models-v2">5.5. Testing other models (v2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-on-other-models-v2">5.6. Evaluation on other models (v2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#intepretation-on-other-models-v2">5.7. Intepretation on other models (v2)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="reproduction_netml.html">6. netML Autogluon</a></li>
<li class="toctree-l1"><a class="reference internal" href="discussion_conclusion.html">7. Conclusion, discussion, and future work</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="pca_dim_reduction.html" title="previous chapter">4. Dimensionality reduction with PCA</a></li>
      <li>Next: <a href="reproduction_netml.html" title="next chapter">6. netML Autogluon</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Rajat Khandelwal.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/models_and_training.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>